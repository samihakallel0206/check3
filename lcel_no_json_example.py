# lcel_no_json_example.py

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_ollama import ChatOllama
import re

# 1) D√©composeur : demander √† LLM de renvoyer des sous-questions num√©rot√©es comme :
# "1. ...\n2. ...\n3. ..."
decompose_prompt = PromptTemplate.from_template(
    "Divisez la question en jusqu'√† 3 sous-questions concises. "
    "Renvoyez-les sous forme de liste num√©rot√©e (1., 2., 3.), rien d'autre.\n\nQuestion:\n{question}"
)

decomposer = decompose_prompt | ChatOllama(model="llama3.2", temperature=0.0)


def parse_numbered_subquestions(base_msg):
    text = getattr(base_msg, "content", str(base_msg)).strip()
    # trouver les lignes commen√ßant par "1." ou "1)"
    lines = re.split(r"\r?\n", text)
    subqs = []
    for line in lines:
        m = re.match(r"\s*\d+\s*[.)]\s*(.*\S.*)$", line)
        if m:
            subqs.append(m.group(1).strip())
    # solution de repli : si aucune correspondance n'est trouv√©e, traiter le texte entier comme une seule sous-question
    if not subqs and text:
        subqs = [text]
    return subqs


parse_subq_runnable = RunnableLambda(parse_numbered_subquestions)

# 2) R√©pondez √† chaque sous-question en renvoyant du texte brut avec des lignes d'√©tiquette :
# "R√©ponse : <r√©ponse en une ligne>\n√âtapes :\n- √©tape 1\n- √©tape 2"
answer_prompt = PromptTemplate.from_template(
    "Vous √™tes un assistant concis. Pour la sous-question ci-dessous, veuillez fournir:\n"
    "R√©ponse: <r√©ponse en une ligne>\n√âtapes:\n- <√©tape 1>\n- <√©tape 2>\nSoyez bref.\n\nSous-question: {subq}"
)

answer_chain = answer_prompt | ChatOllama(model="llama3.2", temperature=0.2)


def run_answers(subquestions):
    # Appel s√©quentiel (plus stable avec Ollama local)
    parsed = []
    for i, q in enumerate(subquestions, 1):
        print(f"  ‚Üí Traitement sous-question {i}/{len(subquestions)}...")
        out = answer_chain.invoke({"subq": q})
        text = getattr(out, "content", str(out)).strip()
        # extraire les lignes de r√©ponse et les puces des √©tapes
        answer_line = None
        steps = []
        for line in text.splitlines():
            if line.lower().startswith("r√©ponse:"):
                answer_line = line.split(":", 1)[1].strip()
            elif re.match(r"\s*[-‚Ä¢]\s+", line):
                steps.append(re.sub(r"^\s*[-‚Ä¢]\s+", "", line).strip())
        # solution de repli : si rien n'est analys√©, conserver le texte brut
        parsed.append({
            "answer": answer_line or text,
            "steps": steps or ["(aucune √©tape analys√©e)"],
            "raw": text
        })
    return parsed


run_answers_runnable = RunnableLambda(run_answers)

# 3) Combinaison : synth√©tiser la r√©ponse courte finale √† partir de la liste des sous-r√©ponses simples
combine_prompt = PromptTemplate.from_template(
    "Synth√©tisez une seule r√©ponse finale concise √† partir de ces sous-r√©ponses num√©rot√©es.\n\n"
    "Entr√©e (chaque √©l√©ment est de type 'R√©ponse: ...' et '√âtapes: - ...'):\n{subanswers_text}\n\n"
    "Veuillez renvoyer exactement trois lignes:\n"
    "1) R√©ponse finale: <une ligne>\n"
    "2) Points cl√©s: - <p1>; - <p2>\n"
    "3) Niveau de confiance: <faible/moyen/√©lev√©>"
)


# Fonction auxiliaire pour formater les sous-r√©ponses en un bloc de texte brut
def format_subanswers_block(ans_list):
    blocks = []
    for i, a in enumerate(ans_list, start=1):
        blocks.append(f"{i}. R√©ponse: {a['answer']}")
        blocks.append("   √âtapes:")
        for s in a["steps"]:
            blocks.append(f"   - {s}")
    return "\n".join(blocks)


format_runnable = RunnableLambda(lambda answers: {"subanswers_text": format_subanswers_block(answers)})
combiner = format_runnable | combine_prompt | ChatOllama(model="llama3.2", temperature=0.0)

# Composition du pipeline
pipeline = decomposer | parse_subq_runnable | run_answers_runnable | combiner

if __name__ == "__main__":
    q = "Comment am√©liorer la s√©curit√© d'une API REST qui g√®re des donn√©es sensibles?"
    
    print("=" * 60)
    print("PIPELINE LCEL - D√©monstration")
    print("=" * 60)
    print(f"\nüìù Question: {q}\n")
    
    print("-" * 60)
    print("√âtape 1: D√©composition en sous-questions...")
    decomposed = decomposer.invoke({"question": q})
    subqs = parse_numbered_subquestions(decomposed)
    print("\nüìã Sous-questions:")
    for i, sq in enumerate(subqs, 1):
        print(f"   {i}. {sq}")
    
    print("\n" + "-" * 60)
    print("√âtape 2: R√©ponse √† chaque sous-question...")
    answers = run_answers(subqs)
    print("\nüìù Sous-r√©ponses:")
    for i, a in enumerate(answers, 1):
        print(f"\n   {i}. R√©ponse: {a['answer']}")
        print("      √âtapes:")
        for s in a["steps"]:
            print(f"        - {s}")
    
    print("\n" + "-" * 60)
    print("√âtape 3: Synth√®se finale...")
    final = combiner.invoke(answers)
    
    print("\n" + "=" * 60)
    print("üéØ R√âSULTAT FINAL:")
    print("=" * 60)
    print(getattr(final, "content", str(final)))
